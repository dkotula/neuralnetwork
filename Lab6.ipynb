{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "vJyLQppSx_UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "XU-HooyIyApB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X / np.max(X)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "Fti8z4FlyBzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(y.reshape(-1, 1)).toarray()"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "mpzf-P92yDD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "NYWRo1juyFHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_entropy_loss(y_true, y_pred):\n",
        " # avoid numerical instability by adding a small constant to log\n",
        " eps = 1e-15\n",
        " y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        " loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        " return loss"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "GFr6OJwTyReg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_categorical_cross_entropy_loss(y_true, y_pred):\n",
        " return (y_pred - y_true) / y_true.shape[0]"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "3JtRfpMWyUqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        " return np.exp(x) / np.sum(np.exp(x))"
      ],
      "outputs": [],
      "execution_count": 165,
      "metadata": {
        "id": "JTn3hQq0yWIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_softmax(x):\n",
        " return x * (1 - x)"
      ],
      "outputs": [],
      "execution_count": 163,
      "metadata": {
        "id": "5S6sT-nxyXaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        " return np.maximum(0, x)"
      ],
      "outputs": [],
      "execution_count": 152,
      "metadata": {
        "id": "H4_TE2EbyY1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_relu(x):\n",
        "  x_copy = np.copy(x)\n",
        "  for index in range(len(x_copy[0])):\n",
        "    if x_copy[0][index] > 0:\n",
        "      x_copy[0][index] = 1\n",
        "  return x_copy"
      ],
      "outputs": [],
      "execution_count": 159,
      "metadata": {
        "id": "pI5FXwVByuIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L1_reg(lambda_, W1, W2):\n",
        " return lambda_ * (np.sum(np.abs(W1)) + np.sum(np.abs(W2)))"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "Twc3qyHpywoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_L1_reg(lambda_, W):\n",
        " return lambda_ * np.sign(W)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "g1X3adfVyxvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L2_reg(lambda_, W1, W2):\n",
        " return lambda_ * (np.sum(np.square(W1)) + np.sum(np.square(W2)))"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "id": "Yw2rFrQMyzAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_L2_reg(lambda_, W):\n",
        " return lambda_ * 2 * W"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "id": "YgHiNhfdy0K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, W1, b1, W2, b2, learning_rate, batch_size, l1_lambda, l2_lambda):\n",
        "  loss = 0.0\n",
        "  for x_el, y_el in zip(X, y):\n",
        "    sum1 = np.dot(x_el, W1) + b1\n",
        "    relu1 = relu(sum1)\n",
        "    sum2 = np.dot(relu1, W2) + b2\n",
        "    y_pred = softmax(sum2)\n",
        "\n",
        "    L1 = np.sum(np.square(W2))\n",
        "    loss += categorical_cross_entropy_loss(y_el, y_pred) + L1_reg(l1_lambda, W1, W2) + L2_reg(l2_lambda, W1, W2)\n",
        "\n",
        "    loss_w2 = relu1.T * (derivative_categorical_cross_entropy_loss(y_el, y_pred) * derivative_softmax(y_pred)) + derivative_L1_reg(l2_lambda, W2) + derivative_L2_reg(l2_lambda, W2)\n",
        "    loss_b2 = derivative_categorical_cross_entropy_loss(y_el, y_pred) * derivative_softmax(y_pred)\n",
        "\n",
        "    W2 -= learning_rate * loss_w2\n",
        "    b2 -= learning_rate * loss_b2 # without regularization\n",
        "\n",
        "    loss_w1 = x_el.reshape(4, 1) * np.sum(W2 * (derivative_categorical_cross_entropy_loss(y_el, y_pred) * derivative_softmax(y_pred)), axis=1).reshape(1, 32) * derivative_relu(relu1) + derivative_L1_reg(l1_lambda, W1) +  + derivative_L2_reg(l1_lambda, W1)\n",
        "    loss_b1 = np.sum(W2 * (derivative_categorical_cross_entropy_loss(y_el, y_pred) * derivative_softmax(y_pred)), axis=1).reshape(1, 32) * derivative_relu(relu1)\n",
        "\n",
        "    W1 -= learning_rate * loss_w1\n",
        "    b1 -= learning_rate * loss_b1 # without regularization\n",
        "\n",
        "  loss /= len(y)\n",
        "  return W1, b1, W2, b2, loss"
      ],
      "outputs": [],
      "execution_count": 167,
      "metadata": {
        "id": "SCTTnIO32fRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, num_epochs, learning_rate, batch_size, l1_lambda, l2_lambda):\n",
        "  input_size = X_train.shape[1] # input layer size\n",
        "  hidden_size = 32 # hidden layer size\n",
        "  output_size = y_train.shape[1] # output layer size\n",
        "\n",
        "  W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "  b1 = np.zeros((1, hidden_size))\n",
        "  W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "  b2 = np.zeros((1, output_size))\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    W1, b1, W2, b2, loss = stochastic_gradient_descent(X, y, W1, b1, W2, b2, learning_rate, batch_size, l1_lambda, l2_lambda)\n",
        "    print('Epoch ' + str(i + 1) + ' Loss: ' + str(loss))\n",
        "  return W1, b1, W2, b2"
      ],
      "outputs": [],
      "execution_count": 144,
      "metadata": {
        "id": "sw78BWWD2kLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "num_epochs = 1000\n",
        "batch_size = 16\n",
        "l1_lambda = 0.001\n",
        "l2_lambda = 0.001\n",
        "\n",
        "W1, b1, W2, b2 = train(X_train, y_train, num_epochs, learning_rate, batch_size, l1_lambda, l2_lambda)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1 Loss: 0.36809905026910344\nEpoch 2 Loss: 0.367112086662052\nEpoch 3 Loss: 0.36682341607697055\nEpoch 4 Loss: 0.366751811272483\nEpoch 5 Loss: 0.3667206449281527\nEpoch 6 Loss: 0.3667007996019491\nEpoch 7 Loss: 0.3666871500129614\nEpoch 8 Loss: 0.3666776631081533\nEpoch 9 Loss: 0.3666709964718058\nEpoch 10 Loss: 0.36666625838510314\nEpoch 11 Loss: 0.36666285107050944\nEpoch 12 Loss: 0.36666037118740535\nEpoch 13 Loss: 0.36665854645223006\nEpoch 14 Loss: 0.36665718804856223\nEpoch 15 Loss: 0.36665616525632677\nEpoch 16 Loss: 0.3666553876120174\nEpoch 17 Loss: 0.36665479132246254\nEpoch 18 Loss: 0.36665432991214203\nEpoch 19 Loss: 0.36665397043087433\nEpoch 20 Loss: 0.36665368835432705\nEpoch 21 Loss: 0.3666534660084892\nEpoch 22 Loss: 0.36665328967270844\nEpoch 23 Loss: 0.36665314930349313\nEpoch 24 Loss: 0.3666530369223604\nEpoch 25 Loss: 0.3666529471847328\nEpoch 26 Loss: 0.36665287595347823\nEpoch 27 Loss: 0.3666528180731596\nEpoch 28 Loss: 0.3666527714534747\nEpoch 29 Loss: 0.3666527338362996\nEpoch 30 Loss: 0.3666527038493476\nEpoch 31 Loss: 0.36665267940266044\nEpoch 32 Loss: 0.36665265965048904\nEpoch 33 Loss: 0.3666526435660477\nEpoch 34 Loss: 0.3666526305715091\nEpoch 35 Loss: 0.3666526202139251\nEpoch 36 Loss: 0.3666526114733374\nEpoch 37 Loss: 0.36665260449520787\nEpoch 38 Loss: 0.36665259867925065\nEpoch 39 Loss: 0.3666525938891752\nEpoch 40 Loss: 0.36665259018075325\nEpoch 41 Loss: 0.36665258708385046\nEpoch 42 Loss: 0.3666525846251891\nEpoch 43 Loss: 0.36665258237704934\nEpoch 44 Loss: 0.36665258143627144\nEpoch 45 Loss: 0.36665258023949054\nEpoch 46 Loss: 0.36665257934290263\nEpoch 47 Loss: 0.3666525785055074\nEpoch 48 Loss: 0.36665257850975347\nEpoch 49 Loss: 0.3666525778866086\nEpoch 50 Loss: 0.36665257735235457\nEpoch 51 Loss: 0.36665257676734997\nEpoch 52 Loss: 0.36665257665756273\nEpoch 53 Loss: 0.366652576299366\nEpoch 54 Loss: 0.36665257626352676\nEpoch 55 Loss: 0.36665257592268896\nEpoch 56 Loss: 0.36665257521577965\nEpoch 57 Loss: 0.36665257621826886\nEpoch 58 Loss: 0.3666525758530518\nEpoch 59 Loss: 0.3666525755991766\nEpoch 60 Loss: 0.36665257536444246\nEpoch 61 Loss: 0.3666525743075925\nEpoch 62 Loss: 0.36665257395242085\nEpoch 63 Loss: 0.3666525739398732\nEpoch 64 Loss: 0.36665257419071745\nEpoch 65 Loss: 0.36665257404151486\nEpoch 66 Loss: 0.36665257350230307\nEpoch 67 Loss: 0.36665257319267247\nEpoch 68 Loss: 0.36665257323723355\nEpoch 69 Loss: 0.3666525731775712\nEpoch 70 Loss: 0.3666525729907121\nEpoch 71 Loss: 0.3666525727352997\nEpoch 72 Loss: 0.366652572904857\nEpoch 73 Loss: 0.3666525734909877\nEpoch 74 Loss: 0.3666525742549378\nEpoch 75 Loss: 0.366652574669538\nEpoch 76 Loss: 0.3666525741411312\nEpoch 77 Loss: 0.3666525749230643\nEpoch 78 Loss: 0.36665257486869174\nEpoch 79 Loss: 0.36665257483376906\nEpoch 80 Loss: 0.3666525747481289\nEpoch 81 Loss: 0.36665257394533474\nEpoch 82 Loss: 0.3666525751299424\nEpoch 83 Loss: 0.36665257496924775\nEpoch 84 Loss: 0.3666525748094184\nEpoch 85 Loss: 0.3666525746395924\nEpoch 86 Loss: 0.36665257466492185\nEpoch 87 Loss: 0.3666525742291776\nEpoch 88 Loss: 0.3666525749774408\nEpoch 89 Loss: 0.36665257468268114\nEpoch 90 Loss: 0.36665257515539085\nEpoch 91 Loss: 0.3666525752025355\nEpoch 92 Loss: 0.3666525751325467\nEpoch 93 Loss: 0.3666525747859021\nEpoch 94 Loss: 0.36665257597841283\nEpoch 95 Loss: 0.36665257614645747\nEpoch 96 Loss: 0.3666525760911666\nEpoch 97 Loss: 0.36665257658515843\nEpoch 98 Loss: 0.3666525756383335\nEpoch 99 Loss: 0.36665257689626446\nEpoch 100 Loss: 0.36665257575639204\nEpoch 101 Loss: 0.3666525752963015\nEpoch 102 Loss: 0.36665257521738975\nEpoch 103 Loss: 0.36665257490155395\nEpoch 104 Loss: 0.3666525744646117\nEpoch 105 Loss: 0.3666525739264879\nEpoch 106 Loss: 0.36665257535542845\nEpoch 107 Loss: 0.36665257515377636\nEpoch 108 Loss: 0.3666525749944834\nEpoch 109 Loss: 0.3666525758774857\nEpoch 110 Loss: 0.36665257632810877\nEpoch 111 Loss: 0.3666525759520856\nEpoch 112 Loss: 0.36665257620574315\nEpoch 113 Loss: 0.3666525754367471\nEpoch 114 Loss: 0.36665257531063605\nEpoch 115 Loss: 0.3666525751759479\nEpoch 116 Loss: 0.3666525749335732\nEpoch 117 Loss: 0.366652574607376\nEpoch 118 Loss: 0.36665257407825635\nEpoch 119 Loss: 0.3666525746365058\nEpoch 120 Loss: 0.36665257570313287\nEpoch 121 Loss: 0.36665257548263097\nEpoch 122 Loss: 0.3666525756157515\nEpoch 123 Loss: 0.3666525753116076\nEpoch 124 Loss: 0.3666525752885419\nEpoch 125 Loss: 0.3666525753254741\nEpoch 126 Loss: 0.36665257524758804\nEpoch 127 Loss: 0.3666525752329879\nEpoch 128 Loss: 0.3666525746280401\nEpoch 129 Loss: 0.3666525755277659\nEpoch 130 Loss: 0.3666525758611304\nEpoch 131 Loss: 0.36665257565290943\nEpoch 132 Loss: 0.366652575026598\nEpoch 133 Loss: 0.36665257589066735\nEpoch 134 Loss: 0.3666525763214316\nEpoch 135 Loss: 0.3666525760563596\nEpoch 136 Loss: 0.3666525758987758\nEpoch 137 Loss: 0.36665257608562185\nEpoch 138 Loss: 0.3666525764633143\nEpoch 139 Loss: 0.36665257627136216\nEpoch 140 Loss: 0.36665257448237304\nEpoch 141 Loss: 0.3666525773250164\nEpoch 142 Loss: 0.36665257846070176\nEpoch 143 Loss: 0.3666525788816288\nEpoch 144 Loss: 0.366652580261738\nEpoch 145 Loss: 0.366652578470832\nEpoch 146 Loss: 0.36665257876506047\nEpoch 147 Loss: 0.3666525777882368\nEpoch 148 Loss: 0.36665257738276336\nEpoch 149 Loss: 0.3666525775182519\nEpoch 150 Loss: 0.3666525774178119\nEpoch 151 Loss: 0.36665257744157354\nEpoch 152 Loss: 0.36665257725176326\nEpoch 153 Loss: 0.36665257813798097\nEpoch 154 Loss: 0.3666525781072093\nEpoch 155 Loss: 0.3666525780911708\nEpoch 156 Loss: 0.3666525772669168\nEpoch 157 Loss: 0.3666525767071318\nEpoch 158 Loss: 0.3666525762724424\nEpoch 159 Loss: 0.36665257583670724\nEpoch 160 Loss: 0.3666525760291299\nEpoch 161 Loss: 0.3666525761779602\nEpoch 162 Loss: 0.3666525765465194\nEpoch 163 Loss: 0.3666525770198952\nEpoch 164 Loss: 0.3666525790427381\nEpoch 165 Loss: 0.3666525797789831\nEpoch 166 Loss: 0.3666525804642338\nEpoch 167 Loss: 0.3666525806643313\nEpoch 168 Loss: 0.36665258013760743\nEpoch 169 Loss: 0.366652580733248\nEpoch 170 Loss: 0.3666525804957057\nEpoch 171 Loss: 0.36665257938445744\nEpoch 172 Loss: 0.36665257805321977\nEpoch 173 Loss: 0.36665257966087395\nEpoch 174 Loss: 0.36665258031973125\nEpoch 175 Loss: 0.36665258108918525\nEpoch 176 Loss: 0.36665258102968246\nEpoch 177 Loss: 0.3666525809106599\nEpoch 178 Loss: 0.3666525805393661\nEpoch 179 Loss: 0.3666525807252071\nEpoch 180 Loss: 0.36665258112206783\nEpoch 181 Loss: 0.36665257960047176\nEpoch 182 Loss: 0.3666525793930094\nEpoch 183 Loss: 0.36665257900892306\nEpoch 184 Loss: 0.3666525790884106\nEpoch 185 Loss: 0.36665257917804606\nEpoch 186 Loss: 0.36665257927198597\nEpoch 187 Loss: 0.36665257928955813\nEpoch 188 Loss: 0.3666525791809044\nEpoch 189 Loss: 0.36665257939770474\nEpoch 190 Loss: 0.3666525797866582\nEpoch 191 Loss: 0.36665258032501186\nEpoch 192 Loss: 0.36665258041118776\nEpoch 193 Loss: 0.36665258017535707\nEpoch 194 Loss: 0.3666525800984437\nEpoch 195 Loss: 0.3666525800147468\nEpoch 196 Loss: 0.3666525802405634\nEpoch 197 Loss: 0.3666525808319702\nEpoch 198 Loss: 0.3666525815838277\nEpoch 199 Loss: 0.3666525815994364\nEpoch 200 Loss: 0.36665258071847584\nEpoch 201 Loss: 0.3666525821164834\nEpoch 202 Loss: 0.3666525826747681\nEpoch 203 Loss: 0.36665258212970486\nEpoch 204 Loss: 0.36665258112815635\nEpoch 205 Loss: 0.36665258121468686\nEpoch 206 Loss: 0.36665258283835744\nEpoch 207 Loss: 0.36665258057590294\nEpoch 208 Loss: 0.3666525803731887\nEpoch 209 Loss: 0.36665258146610086\nEpoch 210 Loss: 0.366652582926811\nEpoch 211 Loss: 0.3666525833266886\nEpoch 212 Loss: 0.3666525835375961\nEpoch 213 Loss: 0.36665258270048545\nEpoch 214 Loss: 0.3666525823861169\nEpoch 215 Loss: 0.36665258312362053\nEpoch 216 Loss: 0.36665258153142455\nEpoch 217 Loss: 0.36665258098798653\nEpoch 218 Loss: 0.36665258037743603\nEpoch 219 Loss: 0.36665258054265776\nEpoch 220 Loss: 0.36665258041208404\nEpoch 221 Loss: 0.3666525802848998\nEpoch 222 Loss: 0.366652580036312\nEpoch 223 Loss: 0.36665258011933083\nEpoch 224 Loss: 0.36665258024129704\nEpoch 225 Loss: 0.366652579685308\nEpoch 226 Loss: 0.3666525781257329\nEpoch 227 Loss: 0.366652577305973\nEpoch 228 Loss: 0.3666525778871539\nEpoch 229 Loss: 0.3666525780246781\nEpoch 230 Loss: 0.36665257806229246\nEpoch 231 Loss: 0.36665257799712947\nEpoch 232 Loss: 0.3666525756882525\nEpoch 233 Loss: 0.3666525764708136\nEpoch 234 Loss: 0.3666525763671744\nEpoch 235 Loss: 0.36665257650882754\nEpoch 236 Loss: 0.3666525769507794\nEpoch 237 Loss: 0.3666525755718078\nEpoch 238 Loss: 0.36665257546661323\nEpoch 239 Loss: 0.36665257486409275\nEpoch 240 Loss: 0.3666525756914808\nEpoch 241 Loss: 0.3666525748146585\nEpoch 242 Loss: 0.36665257567780296\nEpoch 243 Loss: 0.3666525765417321\nEpoch 244 Loss: 0.36665257584892785\nEpoch 245 Loss: 0.36665257599740586\nEpoch 246 Loss: 0.3666525768395069\nEpoch 247 Loss: 0.36665257766184567\nEpoch 248 Loss: 0.3666525757932222\nEpoch 249 Loss: 0.3666525760847835\nEpoch 250 Loss: 0.3666525779962276\nEpoch 251 Loss: 0.36665257787192657\nEpoch 252 Loss: 0.3666525777034416\nEpoch 253 Loss: 0.3666525774247461\nEpoch 254 Loss: 0.36665257540650953\nEpoch 255 Loss: 0.366652577814873\nEpoch 256 Loss: 0.3666525781768409\nEpoch 257 Loss: 0.36665257708187193\nEpoch 258 Loss: 0.36665257681638075\nEpoch 259 Loss: 0.3666525770127088\nEpoch 260 Loss: 0.3666525756924734\nEpoch 261 Loss: 0.36665257520811423\nEpoch 262 Loss: 0.3666525743393834\nEpoch 263 Loss: 0.36665257471217816\nEpoch 264 Loss: 0.36665257697982484\nEpoch 265 Loss: 0.36665257486301545\nEpoch 266 Loss: 0.36665257470582\nEpoch 267 Loss: 0.3666525748916741\nEpoch 268 Loss: 0.36665257225113806\nEpoch 269 Loss: 0.3666525722380357\nEpoch 270 Loss: 0.3666525724931234\nEpoch 271 Loss: 0.3666525716549174\nEpoch 272 Loss: 0.3666525718019309\nEpoch 273 Loss: 0.3666525737138836\nEpoch 274 Loss: 0.36665257506891913\nEpoch 275 Loss: 0.36665257537717866\nEpoch 276 Loss: 0.3666525778514251\nEpoch 277 Loss: 0.3666525782359015\nEpoch 278 Loss: 0.3666525779731249\nEpoch 279 Loss: 0.36665257749130836\nEpoch 280 Loss: 0.36665257715253724\nEpoch 281 Loss: 0.3666525763056637\nEpoch 282 Loss: 0.366652576317993\nEpoch 283 Loss: 0.3666525756075946\nEpoch 284 Loss: 0.3666525779565672\nEpoch 285 Loss: 0.3666525772617327\nEpoch 286 Loss: 0.36665257686888536\nEpoch 287 Loss: 0.3666525781713271\nEpoch 288 Loss: 0.36665257986641253\nEpoch 289 Loss: 0.36665258069982054\nEpoch 290 Loss: 0.366652583749377\nEpoch 291 Loss: 0.36665258530097106\nEpoch 292 Loss: 0.36665258442011095\nEpoch 293 Loss: 0.36665258520890115\nEpoch 294 Loss: 0.3666525851723982\nEpoch 295 Loss: 0.3666525849505412\nEpoch 296 Loss: 0.3666525850047973\nEpoch 297 Loss: 0.36665258492030584\nEpoch 298 Loss: 0.3666525844383985\nEpoch 299 Loss: 0.36665258339875645\nEpoch 300 Loss: 0.36665258335346607\nEpoch 301 Loss: 0.3666525828384472\nEpoch 302 Loss: 0.3666525842269709\nEpoch 303 Loss: 0.3666525859104755\nEpoch 304 Loss: 0.36665258563526226\nEpoch 305 Loss: 0.3666525867331944\nEpoch 306 Loss: 0.366652586570287\nEpoch 307 Loss: 0.36665258522102967\nEpoch 308 Loss: 0.36665258506224646\nEpoch 309 Loss: 0.36665258388595456\nEpoch 310 Loss: 0.3666525836528341\nEpoch 311 Loss: 0.36665258258045325\nEpoch 312 Loss: 0.366652583030699\nEpoch 313 Loss: 0.36665258350303276\nEpoch 314 Loss: 0.3666525825381717\nEpoch 315 Loss: 0.36665258110132537\nEpoch 316 Loss: 0.36665258000344697\nEpoch 317 Loss: 0.366652579252501\nEpoch 318 Loss: 0.3666525806451511\nEpoch 319 Loss: 0.3666525817595939\nEpoch 320 Loss: 0.36665257988507316\nEpoch 321 Loss: 0.36665257841267374\nEpoch 322 Loss: 0.3666525778714542\nEpoch 323 Loss: 0.3666525779491093\nEpoch 324 Loss: 0.3666525771399789\nEpoch 325 Loss: 0.3666525784917536\nEpoch 326 Loss: 0.36665257910207666\nEpoch 327 Loss: 0.3666525805062717\nEpoch 328 Loss: 0.36665257885344843\nEpoch 329 Loss: 0.366652576974124\nEpoch 330 Loss: 0.36665257738989193\nEpoch 331 Loss: 0.3666525760914064\nEpoch 332 Loss: 0.36665257764259707\nEpoch 333 Loss: 0.36665258121415534\nEpoch 334 Loss: 0.3666525832309014\nEpoch 335 Loss: 0.3666525863512827\nEpoch 336 Loss: 0.36665258559186\nEpoch 337 Loss: 0.36665258628856295\nEpoch 338 Loss: 0.3666525818909688\nEpoch 339 Loss: 0.36665258248612287\nEpoch 340 Loss: 0.366652582765477\nEpoch 341 Loss: 0.366652582037989\nEpoch 342 Loss: 0.36665258163584785\nEpoch 343 Loss: 0.3666525809258761\nEpoch 344 Loss: 0.3666525812351184\nEpoch 345 Loss: 0.3666525800220088\nEpoch 346 Loss: 0.3666525796977303\nEpoch 347 Loss: 0.36665258016106733\nEpoch 348 Loss: 0.36665257768317133\nEpoch 349 Loss: 0.3666525749434329\nEpoch 350 Loss: 0.3666525743704549\nEpoch 351 Loss: 0.36665257703349313\nEpoch 352 Loss: 0.3666525761632891\nEpoch 353 Loss: 0.36665257680617985\nEpoch 354 Loss: 0.3666525763383018\nEpoch 355 Loss: 0.3666525744099038\nEpoch 356 Loss: 0.36665257228332904\nEpoch 357 Loss: 0.36665257333855966\nEpoch 358 Loss: 0.3666525745946154\nEpoch 359 Loss: 0.3666525746679744\nEpoch 360 Loss: 0.36665257462816825\nEpoch 361 Loss: 0.36665257770848825\nEpoch 362 Loss: 0.36665257595579476\nEpoch 363 Loss: 0.36665257522523587\nEpoch 364 Loss: 0.36665257456939054\nEpoch 365 Loss: 0.366652574398678\nEpoch 366 Loss: 0.3666525767693661\nEpoch 367 Loss: 0.3666525777952836\nEpoch 368 Loss: 0.36665257956070313\nEpoch 369 Loss: 0.36665258363331577\nEpoch 370 Loss: 0.3666525841613517\nEpoch 371 Loss: 0.3666525833003072\nEpoch 372 Loss: 0.3666525815559441\nEpoch 373 Loss: 0.3666525793258002\nEpoch 374 Loss: 0.36665258109569876\nEpoch 375 Loss: 0.36665257978272014\nEpoch 376 Loss: 0.36665258000181306\nEpoch 377 Loss: 0.3666525792399588\nEpoch 378 Loss: 0.36665258021893543\nEpoch 379 Loss: 0.3666525793176687\nEpoch 380 Loss: 0.36665257902865445\nEpoch 381 Loss: 0.3666525810018838\nEpoch 382 Loss: 0.3666525807627037\nEpoch 383 Loss: 0.3666525802729128\nEpoch 384 Loss: 0.3666525820523826\nEpoch 385 Loss: 0.3666525832296503\nEpoch 386 Loss: 0.3666525828360538\nEpoch 387 Loss: 0.3666525830504206\nEpoch 388 Loss: 0.36665258321390276\nEpoch 389 Loss: 0.3666525825166719\nEpoch 390 Loss: 0.36665257997349937\nEpoch 391 Loss: 0.3666525796981716\nEpoch 392 Loss: 0.36665257980628313\nEpoch 393 Loss: 0.3666525787226063\nEpoch 394 Loss: 0.36665257845378635\nEpoch 395 Loss: 0.3666525771602862\nEpoch 396 Loss: 0.36665257707212234\nEpoch 397 Loss: 0.3666525762558577\nEpoch 398 Loss: 0.3666525763966428\nEpoch 399 Loss: 0.36665257705789817\nEpoch 400 Loss: 0.36665257703781695\nEpoch 401 Loss: 0.3666525774880641\nEpoch 402 Loss: 0.36665257761157577\nEpoch 403 Loss: 0.3666525773121315\nEpoch 404 Loss: 0.3666525860742981\nEpoch 405 Loss: 0.36665258922098076\nEpoch 406 Loss: 0.36665258921132876\nEpoch 407 Loss: 0.3666525897506139\nEpoch 408 Loss: 0.36665258942456974\nEpoch 409 Loss: 0.36665258695343594\nEpoch 410 Loss: 0.3666525917651764\nEpoch 411 Loss: 0.36665259064916855\nEpoch 412 Loss: 0.3666525913376536\nEpoch 413 Loss: 0.3666525893972123\nEpoch 414 Loss: 0.3666525887200331\nEpoch 415 Loss: 0.36665258845174364\nEpoch 416 Loss: 0.36665258489175023\nEpoch 417 Loss: 0.36665258197903844\nEpoch 418 Loss: 0.36665258268638246\nEpoch 419 Loss: 0.366652580953129\nEpoch 420 Loss: 0.36665258383941357\nEpoch 421 Loss: 0.3666525837359017\nEpoch 422 Loss: 0.3666525834353784\nEpoch 423 Loss: 0.3666525841154662\nEpoch 424 Loss: 0.36665258370032877\nEpoch 425 Loss: 0.3666525837282023\nEpoch 426 Loss: 0.3666525818777459\nEpoch 427 Loss: 0.3666525781420317\nEpoch 428 Loss: 0.3666525808459498\nEpoch 429 Loss: 0.36665258142922236\nEpoch 430 Loss: 0.36665258026459513\nEpoch 431 Loss: 0.366652579453422\nEpoch 432 Loss: 0.3666525800681758\nEpoch 433 Loss: 0.3666525814916159\nEpoch 434 Loss: 0.3666525795032366\nEpoch 435 Loss: 0.36665258027896497\nEpoch 436 Loss: 0.3666525806724045\nEpoch 437 Loss: 0.3666525826266375\nEpoch 438 Loss: 0.3666525837244267\nEpoch 439 Loss: 0.3666525822447669\nEpoch 440 Loss: 0.3666525819462125\nEpoch 441 Loss: 0.36665258422383235\nEpoch 442 Loss: 0.3666525835492054\nEpoch 443 Loss: 0.36665258197501094\nEpoch 444 Loss: 0.3666525860452585\nEpoch 445 Loss: 0.3666525921684292\nEpoch 446 Loss: 0.36665259629401736\nEpoch 447 Loss: 0.3666525945359768\nEpoch 448 Loss: 0.36665259408256534\nEpoch 449 Loss: 0.3666525947691351\nEpoch 450 Loss: 0.36665259546494544\nEpoch 451 Loss: 0.3666525961307106\nEpoch 452 Loss: 0.3666525909718991\nEpoch 453 Loss: 0.3666525904935764\nEpoch 454 Loss: 0.3666525897640019\nEpoch 455 Loss: 0.3666525903199384\nEpoch 456 Loss: 0.3666525906016856\nEpoch 457 Loss: 0.36665258800050965\nEpoch 458 Loss: 0.366652589205495\nEpoch 459 Loss: 0.36665259005843404\nEpoch 460 Loss: 0.36665259076840123\nEpoch 461 Loss: 0.36665259054786203\nEpoch 462 Loss: 0.36665259004788714\nEpoch 463 Loss: 0.3666525889831484\nEpoch 464 Loss: 0.3666525897064992\nEpoch 465 Loss: 0.36665259044200205\nEpoch 466 Loss: 0.3666525946294736\nEpoch 467 Loss: 0.3666525895806276\nEpoch 468 Loss: 0.3666525876396334\nEpoch 469 Loss: 0.36665259009212015\nEpoch 470 Loss: 0.3666525919607269\nEpoch 471 Loss: 0.3666525933703638\nEpoch 472 Loss: 0.36665259156760865\nEpoch 473 Loss: 0.36665258914534976\nEpoch 474 Loss: 0.36665259042022164\nEpoch 475 Loss: 0.3666525916556249\nEpoch 476 Loss: 0.36665259120010696\nEpoch 477 Loss: 0.36665259445987025\nEpoch 478 Loss: 0.3666525929484357\nEpoch 479 Loss: 0.3666525905584286\nEpoch 480 Loss: 0.36665258930006883\nEpoch 481 Loss: 0.3666525900214136\nEpoch 482 Loss: 0.3666525868526226\nEpoch 483 Loss: 0.3666525888865836\nEpoch 484 Loss: 0.366652588699985\nEpoch 485 Loss: 0.36665258849188626\nEpoch 486 Loss: 0.36665258815956087\nEpoch 487 Loss: 0.3666525851129629\nEpoch 488 Loss: 0.3666525864540551\nEpoch 489 Loss: 0.3666525868540933\nEpoch 490 Loss: 0.36665258295996106\nEpoch 491 Loss: 0.3666525869202437\nEpoch 492 Loss: 0.3666525919989936\nEpoch 493 Loss: 0.36665259036533365\nEpoch 494 Loss: 0.366652592127223\nEpoch 495 Loss: 0.366652592869966\nEpoch 496 Loss: 0.3666525953743565\nEpoch 497 Loss: 0.3666525972248426\nEpoch 498 Loss: 0.36665259786347226\nEpoch 499 Loss: 0.3666525958968079\nEpoch 500 Loss: 0.3666525930309608\nEpoch 501 Loss: 0.3666525900139417\nEpoch 502 Loss: 0.3666525875622584\nEpoch 503 Loss: 0.3666525913205707\nEpoch 504 Loss: 0.3666525881773488\nEpoch 505 Loss: 0.3666525907213629\nEpoch 506 Loss: 0.3666525955964851\nEpoch 507 Loss: 0.3666525895324176\nEpoch 508 Loss: 0.3666525899747562\nEpoch 509 Loss: 0.3666525913403932\nEpoch 510 Loss: 0.3666525925991456\nEpoch 511 Loss: 0.3666525910236923\nEpoch 512 Loss: 0.36665258894701297\nEpoch 513 Loss: 0.3666525958056395\nEpoch 514 Loss: 0.3666525975460404\nEpoch 515 Loss: 0.36665260126481\nEpoch 516 Loss: 0.36665259897745034\nEpoch 517 Loss: 0.36665260029803814\nEpoch 518 Loss: 0.36665259526493577\nEpoch 519 Loss: 0.36665259772145287\nEpoch 520 Loss: 0.36665259558023044\nEpoch 521 Loss: 0.366652590598183\nEpoch 522 Loss: 0.3666525953919146\nEpoch 523 Loss: 0.36665259246076337\nEpoch 524 Loss: 0.36665259124798116\nEpoch 525 Loss: 0.36665259138533257\nEpoch 526 Loss: 0.3666525897102786\nEpoch 527 Loss: 0.36665259114182513\nEpoch 528 Loss: 0.3666525892315833\nEpoch 529 Loss: 0.36665258909312176\nEpoch 530 Loss: 0.36665259220593094\nEpoch 531 Loss: 0.36665259046736154\nEpoch 532 Loss: 0.3666525935457627\nEpoch 533 Loss: 0.366652595164965\nEpoch 534 Loss: 0.3666525936500809\nEpoch 535 Loss: 0.3666525931517382\nEpoch 536 Loss: 0.36665259155466795\nEpoch 537 Loss: 0.36665258962993036\nEpoch 538 Loss: 0.3666525933569082\nEpoch 539 Loss: 0.366652593616485\nEpoch 540 Loss: 0.3666525926444652\nEpoch 541 Loss: 0.3666525923194218\nEpoch 542 Loss: 0.3666525910140645\nEpoch 543 Loss: 0.3666525966954494\nEpoch 544 Loss: 0.36665259490572716\nEpoch 545 Loss: 0.3666525927108903\nEpoch 546 Loss: 0.3666525880096386\nEpoch 547 Loss: 0.3666525859067949\nEpoch 548 Loss: 0.36665258686404234\nEpoch 549 Loss: 0.3666525887023795\nEpoch 550 Loss: 0.3666525913831689\nEpoch 551 Loss: 0.36665259013005747\nEpoch 552 Loss: 0.36665259207720136\nEpoch 553 Loss: 0.3666525944565041\nEpoch 554 Loss: 0.3666525951965733\nEpoch 555 Loss: 0.36665259258574495\nEpoch 556 Loss: 0.3666525940914552\nEpoch 557 Loss: 0.366652596779442\nEpoch 558 Loss: 0.36665259457454275\nEpoch 559 Loss: 0.3666526012225381\nEpoch 560 Loss: 0.3666526008012559\nEpoch 561 Loss: 0.3666526000040897\nEpoch 562 Loss: 0.3666526010244431\nEpoch 563 Loss: 0.3666526000575067\nEpoch 564 Loss: 0.3666526020638935\nEpoch 565 Loss: 0.36665260313932324\nEpoch 566 Loss: 0.3666526004506254\nEpoch 567 Loss: 0.36665260630488156\nEpoch 568 Loss: 0.3666526044162694\nEpoch 569 Loss: 0.36665260250258214\nEpoch 570 Loss: 0.366652597303076\nEpoch 571 Loss: 0.3666525980164033\nEpoch 572 Loss: 0.36665260108931\nEpoch 573 Loss: 0.36665260691275237\nEpoch 574 Loss: 0.366652601625414\nEpoch 575 Loss: 0.3666526012223555\nEpoch 576 Loss: 0.3666526024036113\nEpoch 577 Loss: 0.36665260696994034\nEpoch 578 Loss: 0.3666526074351412\nEpoch 579 Loss: 0.36665260969029767\nEpoch 580 Loss: 0.36665260979159264\nEpoch 581 Loss: 0.36665261752567085\nEpoch 582 Loss: 0.36665262260659237\nEpoch 583 Loss: 0.3666526152794325\nEpoch 584 Loss: 0.3666526149027352\nEpoch 585 Loss: 0.3666526130573224\nEpoch 586 Loss: 0.36665261148328354\nEpoch 587 Loss: 0.36665260303937725\nEpoch 588 Loss: 0.3666526045305782\nEpoch 589 Loss: 0.36665261059592896\nEpoch 590 Loss: 0.366652609320529\nEpoch 591 Loss: 0.3666526082837341\nEpoch 592 Loss: 0.36665260492028184\nEpoch 593 Loss: 0.36665260016591644\nEpoch 594 Loss: 0.36665260136066463\nEpoch 595 Loss: 0.36665260441967024\nEpoch 596 Loss: 0.3666526118120778\nEpoch 597 Loss: 0.3666526100614907\nEpoch 598 Loss: 0.3666526065255021\nEpoch 599 Loss: 0.36665261020914947\nEpoch 600 Loss: 0.366652608151253\nEpoch 601 Loss: 0.3666526060060671\nEpoch 602 Loss: 0.3666526103074181\nEpoch 603 Loss: 0.36665260978893993\nEpoch 604 Loss: 0.36665260753558787\nEpoch 605 Loss: 0.36665260007152795\nEpoch 606 Loss: 0.3666525994383924\nEpoch 607 Loss: 0.3666526000118736\nEpoch 608 Loss: 0.3666526025123625\nEpoch 609 Loss: 0.3666526044476855\nEpoch 610 Loss: 0.36665260225159024\nEpoch 611 Loss: 0.3666526025374644\nEpoch 612 Loss: 0.3666525992616058\nEpoch 613 Loss: 0.3666526018138498\nEpoch 614 Loss: 0.3666526061574845\nEpoch 615 Loss: 0.36665261450532416\nEpoch 616 Loss: 0.3666526165607319\nEpoch 617 Loss: 0.36665261862698384\nEpoch 618 Loss: 0.36665262152262346\nEpoch 619 Loss: 0.3666526190927317\nEpoch 620 Loss: 0.3666526161956562\nEpoch 621 Loss: 0.36665261587329423\nEpoch 622 Loss: 0.3666526120281374\nEpoch 623 Loss: 0.36665260885944106\nEpoch 624 Loss: 0.3666526100452935\nEpoch 625 Loss: 0.3666526142219235\nEpoch 626 Loss: 0.3666526211738441\nEpoch 627 Loss: 0.3666526217920178\nEpoch 628 Loss: 0.36665261962512824\nEpoch 629 Loss: 0.3666526205947198\nEpoch 630 Loss: 0.3666526133804919\nEpoch 631 Loss: 0.36665261586790365\nEpoch 632 Loss: 0.36665261387381926\nEpoch 633 Loss: 0.36665261087528705\nEpoch 634 Loss: 0.36665261255172477\nEpoch 635 Loss: 0.3666526172537772\nEpoch 636 Loss: 0.3666526160777642\nEpoch 637 Loss: 0.3666526140578679\nEpoch 638 Loss: 0.366652621747646\nEpoch 639 Loss: 0.36665262419138206\nEpoch 640 Loss: 0.3666526203322077\nEpoch 641 Loss: 0.3666526164579867\nEpoch 642 Loss: 0.36665261140078054\nEpoch 643 Loss: 0.3666526258686883\nEpoch 644 Loss: 0.36665263442501195\nEpoch 645 Loss: 0.3666526263653874\nEpoch 646 Loss: 0.3666526217466473\nEpoch 647 Loss: 0.36665262695020895\nEpoch 648 Loss: 0.3666526247360278\nEpoch 649 Loss: 0.36665262525400677\nEpoch 650 Loss: 0.3666526270135723\nEpoch 651 Loss: 0.36665263572853624\nEpoch 652 Loss: 0.36665263495092537\nEpoch 653 Loss: 0.3666526238471685\nEpoch 654 Loss: 0.3666526212474283\nEpoch 655 Loss: 0.36665262698784873\nEpoch 656 Loss: 0.36665263154125305\nEpoch 657 Loss: 0.3666526315082403\nEpoch 658 Loss: 0.36665262558107947\nEpoch 659 Loss: 0.3666526257746809\nEpoch 660 Loss: 0.36665262824608275\nEpoch 661 Loss: 0.3666526302006216\nEpoch 662 Loss: 0.36665264018035565\nEpoch 663 Loss: 0.3666526393884325\nEpoch 664 Loss: 0.36665263102981194\nEpoch 665 Loss: 0.36665263105787704\nEpoch 666 Loss: 0.3666526288159543\nEpoch 667 Loss: 0.36665262845900454\nEpoch 668 Loss: 0.366652620311244\nEpoch 669 Loss: 0.3666526180382409\nEpoch 670 Loss: 0.36665261965458473\nEpoch 671 Loss: 0.3666526275857221\nEpoch 672 Loss: 0.36665263361431344\nEpoch 673 Loss: 0.36665263784593755\nEpoch 674 Loss: 0.366652640276136\nEpoch 675 Loss: 0.3666526375033189\nEpoch 676 Loss: 0.36665263643829044\nEpoch 677 Loss: 0.3666526356490441\nEpoch 678 Loss: 0.36665263037514395\nEpoch 679 Loss: 0.3666526262413116\nEpoch 680 Loss: 0.3666526243909976\nEpoch 681 Loss: 0.3666526232666163\nEpoch 682 Loss: 0.36665263091160116\nEpoch 683 Loss: 0.3666526254184818\nEpoch 684 Loss: 0.3666526245148259\nEpoch 685 Loss: 0.3666526242277644\nEpoch 686 Loss: 0.36665261700510965\nEpoch 687 Loss: 0.3666526157632683\nEpoch 688 Loss: 0.36665262016427\nEpoch 689 Loss: 0.3666526278606551\nEpoch 690 Loss: 0.36665263080927324\nEpoch 691 Loss: 0.366652632410029\nEpoch 692 Loss: 0.3666526369143327\nEpoch 693 Loss: 0.366652639421106\nEpoch 694 Loss: 0.36665264102846506\nEpoch 695 Loss: 0.3666526274275169\nEpoch 696 Loss: 0.3666526282527368\nEpoch 697 Loss: 0.3666526251955562\nEpoch 698 Loss: 0.3666526161673585\nEpoch 699 Loss: 0.3666526141385445\nEpoch 700 Loss: 0.36665259972792574\nEpoch 701 Loss: 0.36665259244883175\nEpoch 702 Loss: 0.3666525968995328\nEpoch 703 Loss: 0.3666525917837812\nEpoch 704 Loss: 0.36665260119235565\nEpoch 705 Loss: 0.3666525917956902\nEpoch 706 Loss: 0.36665259860351657\nEpoch 707 Loss: 0.3666526035890146\nEpoch 708 Loss: 0.3666526079892395\nEpoch 709 Loss: 0.3666526067355203\nEpoch 710 Loss: 0.3666526045887344\nEpoch 711 Loss: 0.366652611047778\nEpoch 712 Loss: 0.36665261591070963\nEpoch 713 Loss: 0.36665261844923275\nEpoch 714 Loss: 0.36665262825440825\nEpoch 715 Loss: 0.36665261788250164\nEpoch 716 Loss: 0.36665261706534064\nEpoch 717 Loss: 0.3666526161407361\nEpoch 718 Loss: 0.3666526165603138\nEpoch 719 Loss: 0.36665259869375\nEpoch 720 Loss: 0.36665259228361785\nEpoch 721 Loss: 0.3666525926107959\nEpoch 722 Loss: 0.36665258804540385\nEpoch 723 Loss: 0.3666525822850726\nEpoch 724 Loss: 0.3666525878498643\nEpoch 725 Loss: 0.36665259175800585\nEpoch 726 Loss: 0.3666525931230604\nEpoch 727 Loss: 0.36665259961582625\nEpoch 728 Loss: 0.36665260430791624\nEpoch 729 Loss: 0.3666526037071455\nEpoch 730 Loss: 0.3666526074963218\nEpoch 731 Loss: 0.3666526214166464\nEpoch 732 Loss: 0.3666526279028292\nEpoch 733 Loss: 0.3666526289480292\nEpoch 734 Loss: 0.366652622892242\nEpoch 735 Loss: 0.3666526209628722\nEpoch 736 Loss: 0.3666526211344609\nEpoch 737 Loss: 0.3666526100079238\nEpoch 738 Loss: 0.36665260602509825\nEpoch 739 Loss: 0.3666526027522682\nEpoch 740 Loss: 0.36665260043797776\nEpoch 741 Loss: 0.366652604547541\nEpoch 742 Loss: 0.3666526010664359\nEpoch 743 Loss: 0.366652602876802\nEpoch 744 Loss: 0.36665259883371776\nEpoch 745 Loss: 0.3666525918664203\nEpoch 746 Loss: 0.36665259476107703\nEpoch 747 Loss: 0.3666525917098102\nEpoch 748 Loss: 0.3666525948292137\nEpoch 749 Loss: 0.3666525986591732\nEpoch 750 Loss: 0.3666526089294199\nEpoch 751 Loss: 0.3666526092235975\nEpoch 752 Loss: 0.3666526077907541\nEpoch 753 Loss: 0.3666526205666005\nEpoch 754 Loss: 0.36665261495102536\nEpoch 755 Loss: 0.36665261402739857\nEpoch 756 Loss: 0.3666526214088363\nEpoch 757 Loss: 0.3666526341817664\nEpoch 758 Loss: 0.3666526333292228\nEpoch 759 Loss: 0.366652654082611\nEpoch 760 Loss: 0.36665264999243147\nEpoch 761 Loss: 0.36665264554749005\nEpoch 762 Loss: 0.36665263404283005\nEpoch 763 Loss: 0.36665262000921145\nEpoch 764 Loss: 0.36665262569852597\nEpoch 765 Loss: 0.36665263428001577\nEpoch 766 Loss: 0.36665264593156077\nEpoch 767 Loss: 0.3666526408071464\nEpoch 768 Loss: 0.366652631544535\nEpoch 769 Loss: 0.36665263550260874\nEpoch 770 Loss: 0.3666526360632439\nEpoch 771 Loss: 0.36665264969425987\nEpoch 772 Loss: 0.3666526308966786\nEpoch 773 Loss: 0.36665261989769554\nEpoch 774 Loss: 0.3666526288018109\nEpoch 775 Loss: 0.36665262190285475\nEpoch 776 Loss: 0.36665262523799974\nEpoch 777 Loss: 0.36665262496746415\nEpoch 778 Loss: 0.3666526229759702\nEpoch 779 Loss: 0.3666526143738586\nEpoch 780 Loss: 0.366652626069779\nEpoch 781 Loss: 0.36665263034766177\nEpoch 782 Loss: 0.3666526296925716\nEpoch 783 Loss: 0.3666526338135001\nEpoch 784 Loss: 0.36665263887723654\nEpoch 785 Loss: 0.36665264013733156\nEpoch 786 Loss: 0.3666526483256121\nEpoch 787 Loss: 0.3666526476628613\nEpoch 788 Loss: 0.3666526366547449\nEpoch 789 Loss: 0.3666526228139456\nEpoch 790 Loss: 0.3666526184619799\nEpoch 791 Loss: 0.3666526287181364\nEpoch 792 Loss: 0.36665261115322273\nEpoch 793 Loss: 0.36665261464121796\nEpoch 794 Loss: 0.36665261540871347\nEpoch 795 Loss: 0.3666526212739781\nEpoch 796 Loss: 0.3666526288972605\nEpoch 797 Loss: 0.36665262495724327\nEpoch 798 Loss: 0.3666526102377057\nEpoch 799 Loss: 0.36665260636538827\nEpoch 800 Loss: 0.3666526055924788\nEpoch 801 Loss: 0.3666526160357003\nEpoch 802 Loss: 0.3666526301895136\nEpoch 803 Loss: 0.3666526311101866\nEpoch 804 Loss: 0.3666526386203339\nEpoch 805 Loss: 0.3666526402546343\nEpoch 806 Loss: 0.3666526162144211\nEpoch 807 Loss: 0.36665262378453756\nEpoch 808 Loss: 0.36665262675615384\nEpoch 809 Loss: 0.3666526276366203\nEpoch 810 Loss: 0.36665261051972736\nEpoch 811 Loss: 0.36665262548499916\nEpoch 812 Loss: 0.3666526236830308\nEpoch 813 Loss: 0.36665262529784964\nEpoch 814 Loss: 0.36665262423352846\nEpoch 815 Loss: 0.3666526210103773\nEpoch 816 Loss: 0.36665262131582615\nEpoch 817 Loss: 0.36665261584652414\nEpoch 818 Loss: 0.3666526263354758\nEpoch 819 Loss: 0.36665262134495286\nEpoch 820 Loss: 0.3666526248702813\nEpoch 821 Loss: 0.3666526224841791\nEpoch 822 Loss: 0.3666526315224245\nEpoch 823 Loss: 0.36665263528894865\nEpoch 824 Loss: 0.3666526339470328\nEpoch 825 Loss: 0.36665263332247944\nEpoch 826 Loss: 0.3666526317880877\nEpoch 827 Loss: 0.3666526401210949\nEpoch 828 Loss: 0.36665265849126805\nEpoch 829 Loss: 0.3666526542153644\nEpoch 830 Loss: 0.3666526557311402\nEpoch 831 Loss: 0.36665266168580196\nEpoch 832 Loss: 0.36665265156303756\nEpoch 833 Loss: 0.36665266037591443\nEpoch 834 Loss: 0.36665267232373855\nEpoch 835 Loss: 0.36665268041797716\nEpoch 836 Loss: 0.36665268867597395\nEpoch 837 Loss: 0.3666526981278144\nEpoch 838 Loss: 0.36665267647650107\nEpoch 839 Loss: 0.36665267384560724\nEpoch 840 Loss: 0.36665268527942835\nEpoch 841 Loss: 0.366652685667705\nEpoch 842 Loss: 0.36665268566187875\nEpoch 843 Loss: 0.36665268403325807\nEpoch 844 Loss: 0.36665267982541494\nEpoch 845 Loss: 0.36665267686954806\nEpoch 846 Loss: 0.3666526782349547\nEpoch 847 Loss: 0.3666526786292667\nEpoch 848 Loss: 0.3666526792834385\nEpoch 849 Loss: 0.3666526645665154\nEpoch 850 Loss: 0.366652667450724\nEpoch 851 Loss: 0.36665267347715286\nEpoch 852 Loss: 0.3666526700435548\nEpoch 853 Loss: 0.36665267530785883\nEpoch 854 Loss: 0.3666526713392859\nEpoch 855 Loss: 0.36665268671457085\nEpoch 856 Loss: 0.36665267468097895\nEpoch 857 Loss: 0.36665268033542636\nEpoch 858 Loss: 0.366652692383544\nEpoch 859 Loss: 0.36665270239499564\nEpoch 860 Loss: 0.36665270912779\nEpoch 861 Loss: 0.36665270158424684\nEpoch 862 Loss: 0.36665268785986793\nEpoch 863 Loss: 0.3666527043815267\nEpoch 864 Loss: 0.36665270995694127\nEpoch 865 Loss: 0.3666526987229332\nEpoch 866 Loss: 0.3666527043777597\nEpoch 867 Loss: 0.36665272502965496\nEpoch 868 Loss: 0.36665272484554623\nEpoch 869 Loss: 0.36665271213942974\nEpoch 870 Loss: 0.36665271053635107\nEpoch 871 Loss: 0.36665269970196024\nEpoch 872 Loss: 0.36665269698005615\nEpoch 873 Loss: 0.3666526947087678\nEpoch 874 Loss: 0.3666526985139749\nEpoch 875 Loss: 0.36665269726067473\nEpoch 876 Loss: 0.3666526827203779\nEpoch 877 Loss: 0.366652691566979\nEpoch 878 Loss: 0.36665268238934695\nEpoch 879 Loss: 0.3666526994152038\nEpoch 880 Loss: 0.3666526917760391\nEpoch 881 Loss: 0.3666526727471323\nEpoch 882 Loss: 0.3666526634358858\nEpoch 883 Loss: 0.36665267777904714\nEpoch 884 Loss: 0.36665268561507364\nEpoch 885 Loss: 0.36665267610650454\nEpoch 886 Loss: 0.3666526726965277\nEpoch 887 Loss: 0.36665266447182404\nEpoch 888 Loss: 0.36665267073887764\nEpoch 889 Loss: 0.3666526554497951\nEpoch 890 Loss: 0.36665263579368834\nEpoch 891 Loss: 0.36665264739766235\nEpoch 892 Loss: 0.3666526560354098\nEpoch 893 Loss: 0.3666526634782322\nEpoch 894 Loss: 0.3666526650912669\nEpoch 895 Loss: 0.36665266679176844\nEpoch 896 Loss: 0.36665266157792065\nEpoch 897 Loss: 0.36665264587288904\nEpoch 898 Loss: 0.36665264864024183\nEpoch 899 Loss: 0.36665267222107645\nEpoch 900 Loss: 0.36665267616281655\nEpoch 901 Loss: 0.3666526631048514\nEpoch 902 Loss: 0.36665265359730903\nEpoch 903 Loss: 0.36665266129928653\nEpoch 904 Loss: 0.3666526666850315\nEpoch 905 Loss: 0.3666526779671913\nEpoch 906 Loss: 0.3666526770140349\nEpoch 907 Loss: 0.3666526997481906\nEpoch 908 Loss: 0.3666526859966098\nEpoch 909 Loss: 0.36665267883585717\nEpoch 910 Loss: 0.3666526792946122\nEpoch 911 Loss: 0.36665267705498705\nEpoch 912 Loss: 0.36665269669570333\nEpoch 913 Loss: 0.36665270422174323\nEpoch 914 Loss: 0.3666527059853209\nEpoch 915 Loss: 0.3666526919641113\nEpoch 916 Loss: 0.3666527057409731\nEpoch 917 Loss: 0.36665271470255667\nEpoch 918 Loss: 0.36665269118200244\nEpoch 919 Loss: 0.366652700029407\nEpoch 920 Loss: 0.36665268998867656\nEpoch 921 Loss: 0.36665270828363966\nEpoch 922 Loss: 0.36665269858256927\nEpoch 923 Loss: 0.3666527050871801\nEpoch 924 Loss: 0.366652697552786\nEpoch 925 Loss: 0.36665270124565424\nEpoch 926 Loss: 0.3666527228809466\nEpoch 927 Loss: 0.36665271456757975\nEpoch 928 Loss: 0.36665271535410715\nEpoch 929 Loss: 0.36665271299048474\nEpoch 930 Loss: 0.3666527224018846\nEpoch 931 Loss: 0.36665273616266797\nEpoch 932 Loss: 0.36665273813778193\nEpoch 933 Loss: 0.36665273430079337\nEpoch 934 Loss: 0.3666527335434938\nEpoch 935 Loss: 0.3666527206180731\nEpoch 936 Loss: 0.36665273323488135\nEpoch 937 Loss: 0.3666527262558827\nEpoch 938 Loss: 0.3666527271328895\nEpoch 939 Loss: 0.3666527219981113\nEpoch 940 Loss: 0.36665271718743514\nEpoch 941 Loss: 0.36665272603677634\nEpoch 942 Loss: 0.36665274511422946\nEpoch 943 Loss: 0.36665275035732453\nEpoch 944 Loss: 0.3666527577168632\nEpoch 945 Loss: 0.3666527830067621\nEpoch 946 Loss: 0.36665276904594213\nEpoch 947 Loss: 0.36665276864997554\nEpoch 948 Loss: 0.36665276967762267\nEpoch 949 Loss: 0.366652754894465\nEpoch 950 Loss: 0.3666527124201039\nEpoch 951 Loss: 0.36665274468817405\nEpoch 952 Loss: 0.36665275259971153\nEpoch 953 Loss: 0.36665275511647694\nEpoch 954 Loss: 0.366652724573522\nEpoch 955 Loss: 0.36665271724708726\nEpoch 956 Loss: 0.3666527421729805\nEpoch 957 Loss: 0.3666527258609541\nEpoch 958 Loss: 0.3666527363795834\nEpoch 959 Loss: 0.366652728885121\nEpoch 960 Loss: 0.36665271990301773\nEpoch 961 Loss: 0.36665269398688966\nEpoch 962 Loss: 0.36665269781646376\nEpoch 963 Loss: 0.36665271354239715\nEpoch 964 Loss: 0.36665273055699377\nEpoch 965 Loss: 0.3666527444285389\nEpoch 966 Loss: 0.3666527483718419\nEpoch 967 Loss: 0.36665274302473855\nEpoch 968 Loss: 0.36665276330901786\nEpoch 969 Loss: 0.36665276005063996\nEpoch 970 Loss: 0.36665275442644457\nEpoch 971 Loss: 0.366652753047778\nEpoch 972 Loss: 0.36665275193693153\nEpoch 973 Loss: 0.3666527527585463\nEpoch 974 Loss: 0.3666527664666531\nEpoch 975 Loss: 0.3666527704501031\nEpoch 976 Loss: 0.36665278010022073\nEpoch 977 Loss: 0.3666527660606262\nEpoch 978 Loss: 0.36665278328391776\nEpoch 979 Loss: 0.36665276716213874\nEpoch 980 Loss: 0.3666527465912149\nEpoch 981 Loss: 0.3666527616197203\nEpoch 982 Loss: 0.36665276645056855\nEpoch 983 Loss: 0.3666528075009529\nEpoch 984 Loss: 0.36665280598704736\nEpoch 985 Loss: 0.3666528273174025\nEpoch 986 Loss: 0.36665281030710345\nEpoch 987 Loss: 0.3666527988933201\nEpoch 988 Loss: 0.36665281091059176\nEpoch 989 Loss: 0.36665280078517337\nEpoch 990 Loss: 0.36665280810283185\nEpoch 991 Loss: 0.36665281393523086\nEpoch 992 Loss: 0.3666528246241022\nEpoch 993 Loss: 0.3666527996745659\nEpoch 994 Loss: 0.36665281047460446\nEpoch 995 Loss: 0.3666527815452125\nEpoch 996 Loss: 0.3666527732967549\nEpoch 997 Loss: 0.3666527623663934\nEpoch 998 Loss: 0.36665276860972235\nEpoch 999 Loss: 0.3666527877196356\nEpoch 1000 Loss: 0.3666528095064679\n"
        }
      ],
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgoudbL_7VJV",
        "outputId": "895f1580-8627-47ac-a70f-fa56532e6321"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error = 0.0\n",
        "for x_el, y_el in zip(X_test, y_test):\n",
        "  sum1 = np.dot(x_el, W1) + b1\n",
        "  relu1 = relu(sum1)\n",
        "  sum2 = np.dot(relu1, W2) + b2\n",
        "  y_pred = softmax(sum2)\n",
        "  error += categorical_cross_entropy_loss(y_el, y_pred)\n",
        "error /= len(np.array(y_test))\n",
        "print('Error over test set: ' + str(error))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Error over test set: 0.36554019338627547\n"
        }
      ],
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD52OVsMtzj",
        "outputId": "5478b7e6-fa24-4ebe-ebf4-f717a6f09da5"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}